{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gan_mnist_supervised.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"P10xnM8hE_na","colab_type":"text"},"cell_type":"markdown","source":["# Supervised GAN example for MNIST dataset\n","Most of the code is similar to gan_mnist.ipynb file except for few modifications on using labels to use on GAN.\n","Credits to Github Repo: https://github.com/soumith/ganhacks\n","on tips and techniques for training GAN models"]},{"metadata":{"id":"fnQcYodLsFek","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":175359,"output_embedded_package_id":"1xhXKZDfOiQ3kXM1qKrUl8knJV5Fe9tPG"},"outputId":"e080178a-a3ab-4ee1-c447-d9c707450999","executionInfo":{"status":"ok","timestamp":1543463952276,"user_tz":300,"elapsed":1025379,"user":{"displayName":"Sangwu Lee","photoUrl":"","userId":"08959789800233677789"}}},"cell_type":"code","source":["#import libraries\n","from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Activation\n","from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt\n","import sys\n","import numpy as np\n","\n","#set optimizer to adam\n","optimizer = Adam(0.0002, 0.5)\n","batch_size = 128\n","\n","#generator model\n","def generator():\n","    #additionally added dropout for regularization\n","    model = Sequential()\n","    model.add(Dense(256, input_dim=(100+1)))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(512))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(1024))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(28*28*1, activation='tanh'))\n","    model.add(Reshape((28,28,1)))\n","    \n","    model.summary()\n","\n","    return model\n","\n","#discriminator model\n","def discriminator():\n","\n","    model = Sequential()\n","\n","    model.add(Flatten(input_shape=(28,28,1)))\n","    model.add(Dense(512))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(256))\n","    model.add(LeakyReLU(alpha=0.2))\n","    model.add(Dropout(0.2))\n","    \n","    inputs = Input(shape=(28,28,1))\n","    features = model(inputs)\n","    \n","    #output both valid_img and valid_label\n","    valid_img = Dense(1,activation='sigmoid')(features) # 0 if fake image, 1 if real image\n","    valid_label = Dense(10+1,activation='softmax')(features) # 0~9 for classfication, 10 for unknown/fake image\n","\n","    return Model(inputs=inputs,outputs=[valid_img,valid_label])\n","\n","#compile models\n","D = discriminator()\n","D.compile(loss=['binary_crossentropy','sparse_categorical_crossentropy'],\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","G = generator()\n","G.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","#100 for random noise and +1 for the label as input\n","inputs = Input(shape=(101,))\n","gen = G(inputs)\n","D.trainable = False\n","target_img,target_label = D(gen)\n","stacked = Model(inputs,[target_img,target_label])\n","\n","#compile stacked model\n","stacked.compile(loss=['binary_crossentropy','sparse_categorical_crossentropy'],\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","#load and normalize data\n","(X_train, y_train), (_, _) = mnist.load_data()\n","X_train = X_train / 127.5 - 1\n","#expand dim to fit shape\n","X_train = np.expand_dims(X_train, axis=3)\n","#1 for valid images, 0 for fake images\n","valid = np.ones((batch_size, 1))\n","fake = np.zeros((batch_size, 1))\n","\n","for epoch in range(50):\n","  for batch in range(int(X_train.shape[0]/batch_size)):\n","    #get images and labels by batch size\n","    imgs = X_train[batch*batch_size:(batch+1)*batch_size]\n","    labels = y_train[batch*batch_size:(batch+1)*batch_size].reshape(batch_size,1)\n","    \n","    #generate noise and add label to generate fake image\n","    noise = np.random.normal(0,1,(batch_size,100))\n","    noise = np.concatenate((noise,labels),axis=1)\n","    fake_imgs = G.predict(noise)\n","    \n","    #train discriminator on both labels and real/fake classification\n","    D.trainable = True\n","    d_loss_real = D.train_on_batch(imgs,[valid,labels]) #use labels for real data\n","    d_loss_fake = D.train_on_batch(fake_imgs,[fake,10*np.ones(labels.shape)]) #use 10 for fake data as labels\n","    \n","    #generate another random noise for training\n","    noise = np.random.normal(0,1,(batch_size,100))\n","    noise = np.concatenate((noise,labels),axis=1)\n","    D.trainable = False #make sure discriminator's weights are fixed during generator's training\n","    g_loss = stacked.train_on_batch(noise, [valid,labels]) #train generator on whole stacked model\n","  print(\"Epoch: \",epoch)\n","  #print 0~9 generated images per epoch to observe performance\n","  for i in range(10):\n","    print(\"Image: \",i)\n","    noise = np.random.normal(0,1,(1,100))\n","    noise = np.concatenate((noise,[[i]]),axis=1)\n","    plt.imshow(G.predict(noise).reshape(28,28))\n","    plt.show()"],"execution_count":1},{"metadata":{"id":"dLS7w4jjHE0_","colab_type":"text"},"cell_type":"markdown","source":["## Results\n","Pretty promising results by the end for simpler digit patterns from 0~5. Especially, some digits are almost indistinguishable from real human written digits. Given further training, noisy output from 6~9 may also benefit from training."]}]}